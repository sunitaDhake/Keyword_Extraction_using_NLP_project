{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Lz65pdcH_X39"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajmgf8OnAm8n",
        "outputId": "4da4ba40-c4dd-4af8-f246-e6459dc54948"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uN1AOLIeR5Mt",
        "outputId": "374747fb-6a8f-4949-cba1-f480cbb94a80"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/drive/My Drive/papers.csv\")\n",
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "x1YBleKPSGFU",
        "outputId": "eb9677f4-bbb3-4628-b710-5af02660679c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id  year                                              title  \\\n",
              "0        1  1987  Self-Organization of Associative Database and ...   \n",
              "1       10  1987  A Mean Field Theory of Layer IV of Visual Cort...   \n",
              "2      100  1988  Storing Covariance by the Associative Long-Ter...   \n",
              "3     1000  1994  Bayesian Query Construction for Neural Network...   \n",
              "4     1001  1994  Neural Network Ensembles, Cross Validation, an...   \n",
              "...    ...   ...                                                ...   \n",
              "7236   994  1994                Single Transistor Learning Synapses   \n",
              "7237   996  1994  Bias, Variance and the Combination of Least Sq...   \n",
              "7238   997  1994          A Real Time Clustering CMOS Neural Engine   \n",
              "7239   998  1994  Learning direction in global motion: two class...   \n",
              "7240   999  1994  Correlation and Interpolation Networks for Rea...   \n",
              "\n",
              "     event_type                                           pdf_name  \\\n",
              "0           NaN  1-self-organization-of-associative-database-an...   \n",
              "1           NaN  10-a-mean-field-theory-of-layer-iv-of-visual-c...   \n",
              "2           NaN  100-storing-covariance-by-the-associative-long...   \n",
              "3           NaN  1000-bayesian-query-construction-for-neural-ne...   \n",
              "4           NaN  1001-neural-network-ensembles-cross-validation...   \n",
              "...         ...                                                ...   \n",
              "7236        NaN        994-single-transistor-learning-synapses.pdf   \n",
              "7237        NaN  996-bias-variance-and-the-combination-of-least...   \n",
              "7238        NaN  997-a-real-time-clustering-cmos-neural-engine.pdf   \n",
              "7239        NaN  998-learning-direction-in-global-motion-two-cl...   \n",
              "7240        NaN  999-correlation-and-interpolation-networks-for...   \n",
              "\n",
              "              abstract                                         paper_text  \n",
              "0     Abstract Missing  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
              "1     Abstract Missing  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
              "2     Abstract Missing  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
              "3     Abstract Missing  Bayesian Query Construction for Neural\\nNetwor...  \n",
              "4     Abstract Missing  Neural Network Ensembles, Cross\\nValidation, a...  \n",
              "...                ...                                                ...  \n",
              "7236  Abstract Missing  Single Transistor Learning Synapses\\n\\nPaul Ha...  \n",
              "7237  Abstract Missing  Bias, Variance and the Combination of\\nLeast S...  \n",
              "7238  Abstract Missing  A Real Time Clustering CMOS\\nNeural Engine\\nT....  \n",
              "7239  Abstract Missing  Learning direction in global motion: two\\nclas...  \n",
              "7240  Abstract Missing  Correlation and Interpolation Networks for\\nRe...  \n",
              "\n",
              "[7241 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-262a83af-b37b-4b3e-9cf4-4349e33a2f37\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>year</th>\n",
              "      <th>title</th>\n",
              "      <th>event_type</th>\n",
              "      <th>pdf_name</th>\n",
              "      <th>abstract</th>\n",
              "      <th>paper_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1987</td>\n",
              "      <td>Self-Organization of Associative Database and ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1-self-organization-of-associative-database-an...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10</td>\n",
              "      <td>1987</td>\n",
              "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>100</td>\n",
              "      <td>1988</td>\n",
              "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bayesian Query Construction for Neural Network...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1001</td>\n",
              "      <td>1994</td>\n",
              "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7236</th>\n",
              "      <td>994</td>\n",
              "      <td>1994</td>\n",
              "      <td>Single Transistor Learning Synapses</td>\n",
              "      <td>NaN</td>\n",
              "      <td>994-single-transistor-learning-synapses.pdf</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Single Transistor Learning Synapses\\n\\nPaul Ha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7237</th>\n",
              "      <td>996</td>\n",
              "      <td>1994</td>\n",
              "      <td>Bias, Variance and the Combination of Least Sq...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>996-bias-variance-and-the-combination-of-least...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Bias, Variance and the Combination of\\nLeast S...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7238</th>\n",
              "      <td>997</td>\n",
              "      <td>1994</td>\n",
              "      <td>A Real Time Clustering CMOS Neural Engine</td>\n",
              "      <td>NaN</td>\n",
              "      <td>997-a-real-time-clustering-cmos-neural-engine.pdf</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>A Real Time Clustering CMOS\\nNeural Engine\\nT....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7239</th>\n",
              "      <td>998</td>\n",
              "      <td>1994</td>\n",
              "      <td>Learning direction in global motion: two class...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>998-learning-direction-in-global-motion-two-cl...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Learning direction in global motion: two\\nclas...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7240</th>\n",
              "      <td>999</td>\n",
              "      <td>1994</td>\n",
              "      <td>Correlation and Interpolation Networks for Rea...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>999-correlation-and-interpolation-networks-for...</td>\n",
              "      <td>Abstract Missing</td>\n",
              "      <td>Correlation and Interpolation Networks for\\nRe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7241 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-262a83af-b37b-4b3e-9cf4-4349e33a2f37')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-262a83af-b37b-4b3e-9cf4-4349e33a2f37 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-262a83af-b37b-4b3e-9cf4-4349e33a2f37');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AF8rcZ6oSbyh",
        "outputId": "03c110ce-b33b-4635-d83a-b96bd63dc7df"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7241, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words=set(stopwords.words(\"english\"))\n",
        "print(stop_words) #return stopwords in set\n",
        "print(len(stop_words))\n",
        "type(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-6GCxcBSdXm",
        "outputId": "b680f450-665b-429c-f805-3882dfb21fc9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'your', 'him', 'then', 've', 'll', 'once', \"shouldn't\", \"it's\", 'myself', 'by', 'couldn', 'nor', 'aren', 'ma', 'where', 'so', 'doing', 'no', 'above', 'any', \"weren't\", 'we', 'being', 'i', 'wasn', 'weren', 'now', 'into', 'the', 'mustn', \"she's\", 'how', \"needn't\", 'be', 'y', 'ourselves', 'only', 'why', 'there', 'has', 'does', 'shan', 'my', 'when', 'were', 'here', 'yourselves', 'did', \"shan't\", \"didn't\", 'd', \"won't\", 'o', 'yourself', 'on', 'doesn', 'hadn', 'his', 'didn', 'an', 'out', 'between', \"hasn't\", \"that'll\", 'until', 'who', 'own', 'which', 'than', 'same', 'but', \"you'll\", 'are', 'that', 'whom', 'more', 'a', 'not', \"isn't\", 'what', 'down', 'our', 'itself', 'themselves', 'having', 'needn', 'most', 'very', 'their', 'wouldn', 'am', \"hadn't\", 'below', 'against', 'me', \"should've\", 'her', 'himself', 'while', 's', 'because', 'few', 'in', 'm', 'and', \"couldn't\", 'she', 'through', 'don', \"don't\", 'some', 'hasn', \"you've\", \"haven't\", 'for', 'this', 'hers', 'from', 'with', 'as', 'them', \"mustn't\", 'before', 'was', 'such', 'its', 'after', 'again', \"you'd\", 'at', 't', \"wasn't\", \"you're\", 'shouldn', 'can', 'these', 'under', 'won', 'about', 'have', 'they', \"mightn't\", 'just', 'should', 'off', \"aren't\", 'during', 'he', 'other', 'those', 'all', 're', 'ours', 'further', 'isn', 'mightn', 'will', 'too', 'ain', \"doesn't\", 'yours', 'each', 'been', 'do', 'had', 'up', 'theirs', 'both', 'you', 'herself', \"wouldn't\", 'if', 'of', 'it', 'or', 'is', 'over', 'to', 'haven'}\n",
            "179\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "set"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_words=[\"fig\",\"figure\",\"image\",\"sample\",\"using\",\"show\",\"result\",\"large\",\"also\",\"one\",\"two\",\"three\",\"four\",\"five\",\"six\",\"seven\",\"eight\",\"nine\"]\n",
        "stop_words=list(stop_words.union(new_words))\n",
        "type(stop_words)"
      ],
      "metadata": {
        "id": "wa5XvHOtTgVZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ebdd0f4-1fd4-49cc-d215-1a86ab0e9663"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DbEpR7veUcfW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing_unit(text):\n",
        "    # create all data of column into lower case first\n",
        "    text=text.lower()\n",
        "\n",
        "    # remove all tags in data of column\n",
        "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
        "\n",
        "    # remove special characters and digits\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "\n",
        "\n",
        "    ##Convert to list from string\n",
        "    text = text.split()    #tokenization (unigrams)\n",
        "\n",
        "    # remove stopwords\n",
        "    text = [word for word in text if word not in stop_words]\n",
        "\n",
        "    # remove words less than three letters\n",
        "    text = [word for word in text if len(word) >= 3]\n",
        "\n",
        "    # lemmatize\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    text = [lmtzr.lemmatize(word) for word in text]\n",
        "\n",
        "    return ' '.join(text)"
      ],
      "metadata": {
        "id": "f3BnAGUAZ-e9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = df['paper_text'].apply(lambda x:preprocessing_unit(x))"
      ],
      "metadata": {
        "id": "zuu7sIjdcNMo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diff=pd.DataFrame({\"paper_text\":df[\"paper_text\"],\"docs\":docs})\n",
        "diff"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "5ceGkRGXdGWW",
        "outputId": "aee8bb9e-dabd-4c62-f1d8-74691b9eddbb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             paper_text  \\\n",
              "0     767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...   \n",
              "1     683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...   \n",
              "2     394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...   \n",
              "3     Bayesian Query Construction for Neural\\nNetwor...   \n",
              "4     Neural Network Ensembles, Cross\\nValidation, a...   \n",
              "...                                                 ...   \n",
              "7236  Single Transistor Learning Synapses\\n\\nPaul Ha...   \n",
              "7237  Bias, Variance and the Combination of\\nLeast S...   \n",
              "7238  A Real Time Clustering CMOS\\nNeural Engine\\nT....   \n",
              "7239  Learning direction in global motion: two\\nclas...   \n",
              "7240  Correlation and Interpolation Networks for\\nRe...   \n",
              "\n",
              "                                                   docs  \n",
              "0     self organization associative database applica...  \n",
              "1     mean field theory layer visual cortex applicat...  \n",
              "2     storing covariance associative long term poten...  \n",
              "3     bayesian query construction neural network mod...  \n",
              "4     neural network ensemble cross validation activ...  \n",
              "...                                                 ...  \n",
              "7236  single transistor learning synapsis paul hasle...  \n",
              "7237  bias variance combination least square estimat...  \n",
              "7238  real time clustering cmos neural engine serran...  \n",
              "7239  learning direction global motion class psychop...  \n",
              "7240  correlation interpolation network real time ex...  \n",
              "\n",
              "[7241 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b0228005-6ff9-426f-b562-256d1203143e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>paper_text</th>\n",
              "      <th>docs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
              "      <td>self organization associative database applica...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
              "      <td>mean field theory layer visual cortex applicat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
              "      <td>storing covariance associative long term poten...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
              "      <td>bayesian query construction neural network mod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
              "      <td>neural network ensemble cross validation activ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7236</th>\n",
              "      <td>Single Transistor Learning Synapses\\n\\nPaul Ha...</td>\n",
              "      <td>single transistor learning synapsis paul hasle...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7237</th>\n",
              "      <td>Bias, Variance and the Combination of\\nLeast S...</td>\n",
              "      <td>bias variance combination least square estimat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7238</th>\n",
              "      <td>A Real Time Clustering CMOS\\nNeural Engine\\nT....</td>\n",
              "      <td>real time clustering cmos neural engine serran...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7239</th>\n",
              "      <td>Learning direction in global motion: two\\nclas...</td>\n",
              "      <td>learning direction global motion class psychop...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7240</th>\n",
              "      <td>Correlation and Interpolation Networks for\\nRe...</td>\n",
              "      <td>correlation interpolation network real time ex...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7241 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b0228005-6ff9-426f-b562-256d1203143e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b0228005-6ff9-426f-b562-256d1203143e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b0228005-6ff9-426f-b562-256d1203143e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YLieLsgDeLsP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv=CountVectorizer(max_df=0.85,max_features=1500,ngram_range=(1,3))\n",
        "\n",
        "\n",
        "word_count_vector=cv.fit_transform(docs)"
      ],
      "metadata": {
        "id": "B1oBda9adpGf"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cv.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7dkGbnL8zHl",
        "outputId": "4a14dc70-d34e-4ac2-9e9e-60b8061f32ae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ability' 'able' 'absolute' ... 'york' 'zero' 'zhang']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(cv.get_feature_names_out())\n",
        "cv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "ghRi4CPWbobK",
        "outputId": "2e2dfa6a-bf56-4f6a-94d6-9175ddd404a3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ability' 'able' 'absolute' ... 'york' 'zero' 'zhang']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(max_df=0.85, max_features=1500, ngram_range=(1, 3))"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(max_df=0.85, max_features=1500, ngram_range=(1, 3))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(max_df=0.85, max_features=1500, ngram_range=(1, 3))</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_cv=pd.DataFrame(data=word_count_vector.toarray(),columns=cv.get_feature_names_out())\n",
        "df_cv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "eqZGhEKxcRZb",
        "outputId": "a8baaf2a-a0a8-4656-958a-c570226a18fb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      ability  able  absolute  access  according  account  accuracy  accurate  \\\n",
              "0           0     0         0       0          0        0         0         0   \n",
              "1           0     0         0       0          1        0         0         0   \n",
              "2           2     0         0       0          0        1         0         0   \n",
              "3           0     0         1       0          5        2         0         0   \n",
              "4           0     1         0       0          0        0         2         0   \n",
              "...       ...   ...       ...     ...        ...      ...       ...       ...   \n",
              "7236        0     0         0       0          2        0         0         0   \n",
              "7237        1     1         0       0          2        0         0         0   \n",
              "7238        0     1         0       0          1        0         0         0   \n",
              "7239        2     1         0       0          0        2         0         0   \n",
              "7240        0     2         0       0          0        0         0         0   \n",
              "\n",
              "      achieve  achieved  ...  would like  write  written  www  year  yet  \\\n",
              "0           1         1  ...           0      1        3    0     0    0   \n",
              "1           0         0  ...           1      2        0    0     0    0   \n",
              "2           0         0  ...           0      0        0    0     0    1   \n",
              "3           0         0  ...           1      0        0    0     0    0   \n",
              "4           0         0  ...           1      0        0    0     0    0   \n",
              "...       ...       ...  ...         ...    ...      ...  ...   ...  ...   \n",
              "7236        2         2  ...           0      0        0    0     2    0   \n",
              "7237        0         0  ...           1      0        1    0     1    0   \n",
              "7238        1         1  ...           0      0        0    0     0    0   \n",
              "7239        0         0  ...           0      0        0    0     0    1   \n",
              "7240        1         1  ...           0      0        0    0     0    1   \n",
              "\n",
              "      yield  york  zero  zhang  \n",
              "0         0     0     0      0  \n",
              "1         0     0     6      0  \n",
              "2         0     1     1      0  \n",
              "3         0     3     2      0  \n",
              "4         2     0     3      0  \n",
              "...     ...   ...   ...    ...  \n",
              "7236      1     0     0      0  \n",
              "7237      2     0     7      0  \n",
              "7238      0     0     0      0  \n",
              "7239      0     0     2      0  \n",
              "7240      2     0     0      0  \n",
              "\n",
              "[7241 rows x 1500 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-589f669d-a53c-45b7-b496-387c215d8cd4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ability</th>\n",
              "      <th>able</th>\n",
              "      <th>absolute</th>\n",
              "      <th>access</th>\n",
              "      <th>according</th>\n",
              "      <th>account</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>accurate</th>\n",
              "      <th>achieve</th>\n",
              "      <th>achieved</th>\n",
              "      <th>...</th>\n",
              "      <th>would like</th>\n",
              "      <th>write</th>\n",
              "      <th>written</th>\n",
              "      <th>www</th>\n",
              "      <th>year</th>\n",
              "      <th>yet</th>\n",
              "      <th>yield</th>\n",
              "      <th>york</th>\n",
              "      <th>zero</th>\n",
              "      <th>zhang</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7236</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7237</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7238</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7239</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7240</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7241 rows × 1500 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-589f669d-a53c-45b7-b496-387c215d8cd4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-589f669d-a53c-45b7-b496-387c215d8cd4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-589f669d-a53c-45b7-b496-387c215d8cd4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "tf_idf_matrix=tfidf_transformer.fit_transform(word_count_vector)"
      ],
      "metadata": {
        "id": "hnToSdpfmENn"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tfidf=pd.DataFrame(tf_idf_matrix.toarray(),columns=cv.get_feature_names_out())\n",
        "df_tfidf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "bfBBnacMW98B",
        "outputId": "783371fb-35c7-4c3c-fcca-179c6ed3273a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       ability      able  absolute  access  according   account  accuracy  \\\n",
              "0     0.000000  0.000000  0.000000     0.0   0.000000  0.000000   0.00000   \n",
              "1     0.000000  0.000000  0.000000     0.0   0.006261  0.000000   0.00000   \n",
              "2     0.017013  0.000000  0.000000     0.0   0.000000  0.008208   0.00000   \n",
              "3     0.000000  0.000000  0.015135     0.0   0.045850  0.023486   0.00000   \n",
              "4     0.000000  0.006173  0.000000     0.0   0.000000  0.000000   0.01321   \n",
              "...        ...       ...       ...     ...        ...       ...       ...   \n",
              "7236  0.000000  0.000000  0.000000     0.0   0.014516  0.000000   0.00000   \n",
              "7237  0.007887  0.006326  0.000000     0.0   0.011887  0.000000   0.00000   \n",
              "7238  0.000000  0.011304  0.000000     0.0   0.010620  0.000000   0.00000   \n",
              "7239  0.020317  0.008148  0.000000     0.0   0.000000  0.019605   0.00000   \n",
              "7240  0.000000  0.014108  0.000000     0.0   0.000000  0.000000   0.00000   \n",
              "\n",
              "      accurate   achieve  achieved  ...  would like     write   written  www  \\\n",
              "0          0.0  0.011141  0.011702  ...    0.000000  0.014473  0.041026  0.0   \n",
              "1          0.0  0.000000  0.000000  ...    0.008996  0.018499  0.000000  0.0   \n",
              "2          0.0  0.000000  0.000000  ...    0.000000  0.000000  0.000000  0.0   \n",
              "3          0.0  0.000000  0.000000  ...    0.013176  0.000000  0.000000  0.0   \n",
              "4          0.0  0.000000  0.000000  ...    0.008333  0.000000  0.000000  0.0   \n",
              "...        ...       ...       ...  ...         ...       ...       ...  ...   \n",
              "7236       0.0  0.016506  0.017338  ...    0.000000  0.000000  0.000000  0.0   \n",
              "7237       0.0  0.000000  0.000000  ...    0.008539  0.000000  0.008296  0.0   \n",
              "7238       0.0  0.012076  0.012684  ...    0.000000  0.000000  0.000000  0.0   \n",
              "7239       0.0  0.000000  0.000000  ...    0.000000  0.000000  0.000000  0.0   \n",
              "7240       0.0  0.007536  0.007916  ...    0.000000  0.000000  0.000000  0.0   \n",
              "\n",
              "          year       yet     yield      york      zero  zhang  \n",
              "0     0.000000  0.000000  0.000000  0.000000  0.000000    0.0  \n",
              "1     0.000000  0.000000  0.000000  0.000000  0.032794    0.0  \n",
              "2     0.000000  0.008989  0.000000  0.008849  0.005595    0.0  \n",
              "3     0.000000  0.000000  0.000000  0.037978  0.016009    0.0  \n",
              "4     0.000000  0.000000  0.012851  0.000000  0.015188    0.0  \n",
              "...        ...       ...       ...       ...       ...    ...  \n",
              "7236  0.023085  0.000000  0.008042  0.000000  0.000000    0.0  \n",
              "7237  0.009452  0.000000  0.013170  0.000000  0.036317    0.0  \n",
              "7238  0.000000  0.000000  0.000000  0.000000  0.000000    0.0  \n",
              "7239  0.000000  0.010735  0.000000  0.000000  0.013364    0.0  \n",
              "7240  0.000000  0.009294  0.014685  0.000000  0.000000    0.0  \n",
              "\n",
              "[7241 rows x 1500 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7fa97a06-2cee-4787-a2d5-b6c808c26a99\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ability</th>\n",
              "      <th>able</th>\n",
              "      <th>absolute</th>\n",
              "      <th>access</th>\n",
              "      <th>according</th>\n",
              "      <th>account</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>accurate</th>\n",
              "      <th>achieve</th>\n",
              "      <th>achieved</th>\n",
              "      <th>...</th>\n",
              "      <th>would like</th>\n",
              "      <th>write</th>\n",
              "      <th>written</th>\n",
              "      <th>www</th>\n",
              "      <th>year</th>\n",
              "      <th>yet</th>\n",
              "      <th>yield</th>\n",
              "      <th>york</th>\n",
              "      <th>zero</th>\n",
              "      <th>zhang</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011141</td>\n",
              "      <td>0.011702</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014473</td>\n",
              "      <td>0.041026</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.006261</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008996</td>\n",
              "      <td>0.018499</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.032794</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.017013</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008208</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008989</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008849</td>\n",
              "      <td>0.005595</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.045850</td>\n",
              "      <td>0.023486</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013176</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.037978</td>\n",
              "      <td>0.016009</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006173</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.01321</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.012851</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.015188</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7236</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.014516</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.016506</td>\n",
              "      <td>0.017338</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.023085</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008042</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7237</th>\n",
              "      <td>0.007887</td>\n",
              "      <td>0.006326</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.011887</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008539</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008296</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.009452</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013170</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.036317</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7238</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.011304</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.010620</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.012076</td>\n",
              "      <td>0.012684</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7239</th>\n",
              "      <td>0.020317</td>\n",
              "      <td>0.008148</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.019605</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010735</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013364</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7240</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.014108</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.007536</td>\n",
              "      <td>0.007916</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009294</td>\n",
              "      <td>0.014685</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7241 rows × 1500 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7fa97a06-2cee-4787-a2d5-b6c808c26a99')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7fa97a06-2cee-4787-a2d5-b6c808c26a99 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7fa97a06-2cee-4787-a2d5-b6c808c26a99');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "pickle.dump(cv,open(\"drive/MyDrive/Dataset_keyword_countvectorizer.pkl\",\"wb\"))\n"
      ],
      "metadata": {
        "id": "QZa-3o5_uUic"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(tfidf_transformer,open(\"drive/MyDrive/tfidf_transformer.pkl\",\"wb\"))"
      ],
      "metadata": {
        "id": "_NCYepasuDgQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sort_coo(coo_matrix):\n",
        "  tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "  return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)"
      ],
      "metadata": {
        "id": "UqfqSc-ibFLA"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J9tFOlJFdBxU"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
        "\n",
        "    #use only topn items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        "\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "\n",
        "    for idx, score in sorted_items:\n",
        "        # fname = feature_names[idx]\n",
        "\n",
        "        #keep track of feature name and its corresponding score\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        "\n",
        "    #create a tuples of feature,score\n",
        "    # results = zip(feature_vals,score_vals)\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "6oEoucHkvu9T"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get feature names\n",
        "feature_names=cv.get_feature_names_out()\n",
        "len(feature_names)\n",
        "feature_names[15]"
      ],
      "metadata": {
        "id": "Hgp75EWxv7RF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "b42da6ff-5c50-4a22-952b-947a82a3961e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'activation'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(feature_names, open('drive/MyDrive/keywords-feature-names.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "28uGLPf5wEK9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names[230:234]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0tLSXrnwEa0",
        "outputId": "c3f044ae-9ef8-49a1-e5e3-ccb32f16547f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['concave', 'concept', 'conclusion', 'condition'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O0FzsL5_bAKM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_keywords(idx, docs):\n",
        "\n",
        "    #generate tf-idf for the given document\n",
        "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs[idx]]))\n",
        "\n",
        "    #sort the tf-idf vectors by descending order of scores\n",
        "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "\n",
        "    #extract only the top n; n here is 10\n",
        "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "\n",
        "    return keywords"
      ],
      "metadata": {
        "id": "mprBuMCQwQR8"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_keywords_text(docs):\n",
        "\n",
        "    #generate tf-idf for the given document\n",
        "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs]))\n",
        "\n",
        "    #sort the tf-idf vectors by descending order of scores\n",
        "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "    #extract only the top n; n here is 10\n",
        "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "\n",
        "    return keywords"
      ],
      "metadata": {
        "id": "0U0pENWQwVTX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[225])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buovDvLBwYyq",
        "outputId": "5962229c-c7d7-4587-d59b-5e4675ba1c76"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "valid generalization size weight important size network peter bartlett department system engineering research school information science engineering australian national university canberra australia peter bartlettclanu edu abstract paper show neural network used pattern classification problem learning algorithm find network small weight small squared error training pattern generalization performance depends size weight rather number weight specifically consider layer feed forward network sigmoid unit sum magnitude weight associated unit bounded misclassification probability converges error estimate closely related squared error training set rate log ignoring log factor number training pattern input dimension constant may explain generalization performance neural network particularly number training example considerably smaller number weight support heuristic weight decay early stopping attempt keep weight small training introduction result statistical learning theory give bound number training example necessary satisfactory generalization performance classification problem term vapnik chervonenkis dimension class function used learning system see example baum haussler used result give size bound multi layer threshold network generalization size weight neural network grow least quickly number weight see however pattern classification application bound seem loose neural network often perform successfully training set considerably smaller number weight paper show classification problem neural network perform well weight big size weight determines generalization performance contrast function class algorithm considered theory neural network used binary classification problem real valued output learning algorithm typically attempt minimize squared error network output training set well encouraging correct classification tends push output away zero towards target value easy see total squared error hypothesis example example hypothesis either incorrect sign magnitude le next section give misclassification probability bound hypothesis distinctly correct way example bound term scale sensitive version dimension called fat shattering dimension section give bound dimension feedforward sigmoid network imply main result proof sketched section full proof found full version notation bound misclassification probability denote space input pattern space label assume probability distribution product space reflects relative frequency different input pattern relative frequency expert classification pattern learning algorithm us class real valued function called hypothesis class hypothesis correct example sgn sgn take value iff misclassification probability error defined erp sgn crucial quantity determining misclassification probability fat shattering dimension hypothesis class say sequence point shattered iffunctions give classification sequence satisfying sgn dimension defined size largest shattered sequence given scale parameter say sequence point shattered sequence real value satisfying rdb fat shattering dimension denoted fath size largest shattered sequence dimension reflects complexity function class examined scale notice fath nonincreasing function following theorem give generalization error bound term fath related applies case error training set appear theorem define input space hypothesis class probability distribution let probability training sequence labelled fact according usual definition dimension class thresholded version function bartlett example every hypothesis satisfies erp xdl sgn log fathb comment informative compare standard bound case bound misclassification probability erp sgn ydl dlog log vcdim constant shall see next section function class vcdim infinite fathb finite example class function computed layer neural network arbitrary number parameter constraint size parameter known learning algorithm error estimate constrained make use considering proportion training example hypothesis misclassify distribution second term bound cannot improved log factor theorem show improved learning algorithm make use considering proportion training example correctly classified xdl possible give lower bound see full paper function class considered show theorem cannot improved log factor idea magnitude value give precise estimate generalization performance first proposed vapnik developed vapnik worker used case linear hypothesis class result give bound misclassification probability test term value training test data extended give bound misclassification probability unseen data term value training example extended general function class give error bound applicable hypothesis error training example lugosi pinter obtained bound misclassification probability term similar property class function containing true regression function conditional expectation given however result extend case true regression function class real valued function used estimator seems unnatural quantity specified advance theorem since depends example full paper give similar statement made uniform value quantity fat shattering dimension neural network bound dimensionofvarious neural network class established see review least linear number parameter section give bound fat shattering dimension several neural network class generalization size ofthe weight neural network assume input space subset define sigmoid unit function parametrized vector weight unit computes fixed bounded function satisfying lipchitz condition simplicity ignore offset parameter equivalent including extra input constant value multi layer feed forward sigmoid network depth network sigmoid unit single output unit arranged layered structure layer output unit pass input unit later layer consider network weight bounded relevant norm norm vector define iiwl iwil following give bound fatshattering dimension bounded linear combination real valued function term fat shattering dimension basis function class apply recursive fashion give bound single output feed forward network theorem let class function map define class weight bounded linear combination function wdi suppose fatfb log constant fathb gurvits koiran shown fat shattering dimension class layer network bounded output weight linear threshold hidden unit log lrn special case theorem improves notice fat shattering dimension function class changed constant factor compose function fixed function satisfying lipschitz condition like standard sigmoid function fathb logn finally fathb observation together theorem give following corollary notation suppresses log factor formally corollary class layer sigmoid network weight outp unit satisfying iiwlh fathb ilxli hidden unit weight bounded fathb log applying theorem give following deeper network notice constraint number hidden unit layer total magnitude weight associated processing unit corollary constant class depth sigmoid network weight vector associated unit beyond first layer satisfies iiwlll fathb iixlioo weight first layer unit satisfy iiwll fathb llog first part corollary network fat shattering dimension similar dimension linear network formalizes intuition weight small network operates linear part sigmoid behaves like linear network bartlett comment consider depth sigmoid network bounded weight last corollary theorem imply training size grows roughly misclassification probability network within proportion training example network classifies distinctly correct result give plausible explanation generalization performance neural network application network many unit small weight small squared error training example dimension hence number parameter important magnitude weight generalization performance possible give version theorem probability bound uniform value complexity parameter indexing function class technique mentioned end section case sigmoid network class indexed weight bound minimizing resulting bound misclassification probability equivalent minimizing sum error term penalty term involving weight bound support use popular heuristic technique weight decay early stopping see example aim minimize squared error maintaining small weight technique give bound fat shattering dimension hence generalization performance function class expressed bounded number composition either bounded weight linear combination scalar lipschitz function function class finite fat shattering dimension includes example radial basis function network proof proof sketch theorem pseudometric space set cover tin define size smallest cover define pseudometric dloo set function defined dloo max set function denote maxxex dloo noo alon obtained following bound noo term fat shattering dimension lemma class function map fatf log noo log log provided log define piecewise linear squashing function satisfying otherwise class real valued function define set composition function lemma theorem erp cnoo orsgn generalization size weight neural network proof lemma relies observation erp sgn ydl ydl use standard symmetrization argument permutation argument introduced vapnik chervonenkis bound probability probability random permutation double length related property hold fixed use pollard approach approximating hypothesis class cover except case appropriate cover respect pseudometric applying hoeffding inequality give lemma prove theorem need bound covering number term fatshattering dimension easy apply lemma quantized version function class get bound taking advantage range constraint imposed squashing function proof sketch theorem define pseudometric class function defined similarly define set function defined denote maxxexm similarly idea proof theorem first derive general upper bound covering number class apply following implicit proof theorem give bound fat shattering dimension lemma class valued function satisfying fatf log derive upper bound start bound lemma implies covering number noo class hidden unit function since implies following bound covering number provided satisfies condition required lemma turn theorem trivial otherwise log dlog emm log next use following approximation barron attribute maurey lemma maurey suppose hilbert space let element convex closure function iii lil implies element approximated particular accuracy respect fixed linear combination small number element follows construct cover cover lemma inequality show log emma dlog log bartlett jensen inequality implies give bound comparing lower bound given lemma solving give refined analysis neural network case involves bounding successive layer solving give bound fat shattering dimension network acknowledgement thanks andrew barron jonathan baxter mike jordan adam kowalczyk wee sun lee phil long john shawe taylor robert slaviero helpful discussion comment reference alon ben david cesa bianchi haussler scale sensitive mensions uniform convergence learn ability proceeding ieee symposium foundation computer science ieee press bartlett complexity pattern classification neural network size weight important size network technical report department system engineering australian national university available anonymous ftp syseng anu edu pub peter bartlett kulkarni posner covering number realvalued function class technical report australian national university princeton university baum haussler size net give valid generalization neural computation blumer ehrenfeucht haussler warmuth learnability vapnik chervorienkis dimension acm gurvits koiran approximation learning convex superposition computational learning theory eurocolt haussler decision theoretic generalization pac model neural net learning application inform comput hertz krogh palmer introduction theory neural computation addison wesley lugosi pinter data dependent skeleton estimate learning proc annu conference comput learning theory acm press new york maass vapnik chervonenkis dimension neural net arbib editor handbook brain theory neural network page mit press cambridge shawe taylor bartlett williamson anthony framework structural risk minimisation proc annu conference comput learning theory acm press new york shawe taylor bartlett williamson anthony structural risk minimization data dependent hierarchy technical report vapnik estimation dependence based empirical data springerverlag new york\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is my testing data.\n",
        "testing = \"valid generalization size weight important size network peter bartlett department system engineering research school information science engineering australian national university canberra australia peter bartlettclanu edu abstract paper show neural network used pattern classification problem learning algorithm find network small weight small squared error training pattern generalization performance depends size weight rather number weight specifically consider layer feed forward network sigmoid unit sum magnitude weight associated unit bounded misclassification probability converges error estimate closely related squared error training set rate log ignoring log factor number training pattern input dimension constant may explain generalization performance neural network particularly number training example considerably smaller number weight support heuristic weight decay early stopping attempt keep weight small training introduction result statistical learning theory give bound number training example necessary satisfactory generalization performance classification problem term vapnik chervonenkis dimension class function used learning system see example baum haussler used result give size bound multi layer threshold network generalization size weight neural network grow least quickly number weight see however pattern classification application bound seem loose neural network often perform successfully training set considerably smaller number weight paper show classification problem neural network perform well weight big size weight determines generalization performance contrast function class algorithm considered theory neural network used binary classification problem real valued output learning algorithm typically attempt minimize squared error network output training set well encouraging correct classification tends push output away zero towards target value easy see total squared error hypothesis example example hypothesis either incorrect sign magnitude le next section give misclassification probability bound hypothesis distinctly correct way example bound term scale sensitive version dimension called fat shattering dimension section give bound dimension feedforward sigmoid network imply main result proof sketched section full proof found full version notation bound misclassification probability denote space input pattern space label assume probability distribution product space reflects relative frequency different input pattern relative frequency expert classification pattern learning algorithm us class real valued function called hypothesis class hypothesis correct example sgn sgn take value iff misclassification probability error defined erp sgn crucial quantity determining misclassification probability fat shattering dimension hypothesis class say sequence point shattered iffunctions give classification sequence satisfying sgn dimension defined size largest shattered sequence given scale parameter say sequence point shattered sequence real value satisfying rdb fat shattering dimension denoted fath size largest shattered sequence dimension reflects complexity function class examined scale notice fath nonincreasing function following theorem give generalization error bound term fath related applies case error training set appear theorem define input space hypothesis class probability distribution let probability training sequence labelled fact according usual definition dimension class thresholded version function bartlett example every hypothesis satisfies erp xdl sgn log fathb comment informative compare standard bound case bound misclassification probability erp sgn ydl dlog log vcdim constant shall see next section function class vcdim infinite fathb finite example class function computed layer neural network arbitrary number parameter constraint size parameter known learning algorithm error estimate constrained make use considering proportion training example hypothesis misclassify distribution second term bound cannot improved log factor theorem show improved learning algorithm make use considering proportion training example correctly classified xdl possible give lower bound see full paper function class considered show theorem cannot improved log factor idea magnitude value give precise estimate generalization performance first proposed vapnik developed vapnik worker used case linear hypothesis class result give bound misclassification probability test term value training test data extended give bound misclassification probability unseen data term value training example extended general function class give error bound applicable hypothesis error training example lugosi pinter obtained bound misclassification probability term similar property class function containing true regression function conditional expectation given however result extend case true regression function class real valued function used estimator seems unnatural quantity specified advance theorem since depends example full paper give similar statement made uniform value quantity fat shattering dimension neural network bound dimensionofvarious neural network class established see review least linear number parameter section give bound fat shattering dimension several neural network class generalization size ofthe weight neural network assume input space subset define sigmoid unit function parametrized vector weight unit computes fixed bounded function satisfying lipchitz condition simplicity ignore offset parameter equivalent including extra input constant value multi layer feed forward sigmoid network depth network sigmoid unit single output unit arranged layered structure layer output unit pass input unit later layer consider network weight bounded relevant norm norm vector define iiwl iwil following give bound fatshattering dimension bounded linear combination real valued function term fat shattering dimension basis function class apply recursive fashion give bound single output feed forward network theorem let class function map define class weight bounded linear combination function wdi suppose fatfb log constant fathb gurvits koiran shown fat shattering dimension class layer network bounded output weight linear threshold hidden unit log lrn special case theorem improves notice fat shattering dimension function class changed constant factor compose function fixed function satisfying lipschitz condition like standard sigmoid function fathb logn finally fathb observation together theorem give following corollary notation suppresses log factor formally corollary class layer sigmoid network weight outp unit satisfying iiwlh fathb ilxli hidden unit weight bounded fathb log applying theorem give following deeper network notice constraint number hidden unit layer total magnitude weight associated processing unit corollary constant class depth sigmoid network weight vector associated unit beyond first layer satisfies iiwlll fathb iixlioo weight first layer unit satisfy iiwll fathb llog first part corollary network fat shattering dimension similar dimension linear network formalizes intuition weight small network operates linear part sigmoid behaves like linear network bartlett comment consider depth sigmoid network bounded weight last corollary theorem imply training size grows roughly misclassification probability network within proportion training example network classifies distinctly correct result give plausible explanation generalization performance neural network application network many unit small weight small squared error training example dimension hence number parameter important magnitude weight generalization performance possible give version theorem probability bound uniform value complexity parameter indexing function class technique mentioned end section case sigmoid network class indexed weight bound minimizing resulting bound misclassification probability equivalent minimizing sum error term penalty term involving weight bound support use popular heuristic technique weight decay early stopping see example aim minimize squared error maintaining small weight technique give bound fat shattering dimension hence generalization performance function class expressed bounded number composition either bounded weight linear combination scalar lipschitz function function class finite fat shattering dimension includes example radial basis function network proof proof sketch theorem pseudometric space set cover tin define size smallest cover define pseudometric dloo set function defined dloo max set function denote maxxex dloo noo alon obtained following bound noo term fat shattering dimension lemma class function map fatf log noo log log provided log define piecewise linear squashing function satisfying otherwise class real valued function define set composition function lemma theorem erp cnoo orsgn generalization size weight neural network proof lemma relies observation erp sgn ydl ydl use standard symmetrization argument permutation argument introduced vapnik chervonenkis bound probability probability random permutation double length related property hold fixed use pollard approach approximating hypothesis class cover except case appropriate cover respect pseudometric applying hoeffding inequality give lemma prove theorem need bound covering number term fatshattering dimension easy apply lemma quantized version function class get bound taking advantage range constraint imposed squashing function proof sketch theorem define pseudometric class function defined similarly define set function defined denote maxxexm similarly idea proof theorem first derive general upper bound covering number class apply following implicit proof theorem give bound fat shattering dimension lemma class valued function satisfying fatf log derive upper bound start bound lemma implies covering number noo class hidden unit function since implies following bound covering number provided satisfies condition required lemma turn theorem trivial otherwise log dlog emm log next use following approximation barron attribute maurey lemma maurey suppose hilbert space let element convex closure function iii lil implies element approximated particular accuracy respect fixed linear combination small number element follows construct cover cover lemma inequality show log emma dlog log bartlett jensen inequality implies give bound comparing lower bound given lemma solving give refined analysis neural network case involves bounding successive layer solving give bound fat shattering dimension network acknowledgement thanks andrew barron jonathan baxter mike jordan adam kowalczyk wee sun lee phil long john shawe taylor robert slaviero helpful discussion comment reference alon ben david cesa bianchi haussler scale sensitive mensions uniform convergence learn ability proceeding ieee symposium foundation computer science ieee press bartlett complexity pattern classification neural network size weight important size network technical report department system engineering australian national university available anonymous ftp syseng anu edu pub peter bartlett kulkarni posner covering number realvalued function class technical report australian national university princeton university baum haussler size net give valid generalization neural computation blumer ehrenfeucht haussler warmuth learnability vapnik chervorienkis dimension acm gurvits koiran approximation learning convex superposition computational learning theory eurocolt haussler decision theoretic generalization pac model neural net learning application inform comput hertz krogh palmer introduction theory neural computation addison wesley lugosi pinter data dependent skeleton estimate learning proc annu conference comput learning theory acm press new york maass vapnik chervonenkis dimension neural net arbib editor handbook brain theory neural network page mit press cambridge shawe taylor bartlett williamson anthony framework structural risk minimisation proc annu conference comput learning theory acm press new york shawe taylor bartlett williamson anthony structural risk minimization data dependent hierarchy technical report vapnik estimation dependence based empirical data springerverlag new york\"\n",
        "\n",
        "keywords_text = get_keywords_text(testing)\n",
        "\n",
        "for k in keywords_text:\n",
        "        print(k, keywords_text[k])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIW9gfRbwZDg",
        "outputId": "dab60072-4408-43cf-b770-bbab55d4b500"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bound 0.332\n",
            "network 0.31\n",
            "class 0.295\n",
            "weight 0.278\n",
            "dimension 0.252\n",
            "give 0.204\n",
            "theorem 0.175\n",
            "hypothesis 0.16\n",
            "unit 0.16\n",
            "generalization 0.157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_results(idx,keywords, df):\n",
        "    # now print the results\n",
        "    print(\"Title\")\n",
        "    print(df['title'][idx])\n",
        "    print(\"\\nAbstract\")\n",
        "    print(df['abstract'][idx])\n",
        "    print(\"\\nKeywords for this text\")\n",
        "    for k in keywords:\n",
        "        print(k,keywords[k])"
      ],
      "metadata": {
        "id": "ID9jTgXlwQpi"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx=120\n",
        "keywords=get_keywords(idx, docs)\n",
        "print_results(idx,keywords, df)"
      ],
      "metadata": {
        "id": "RRtrvPtVxI77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2deb9623-ae34-4764-9ce9-ae076c019e42"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title\n",
            "Human Reading and the Curse of Dimensionality\n",
            "\n",
            "Abstract\n",
            "Abstract Missing\n",
            "\n",
            "Keywords for this text\n",
            "character 0.769\n",
            "word 0.322\n",
            "dimensionality 0.192\n",
            "window 0.128\n",
            "training 0.124\n",
            "eye 0.121\n",
            "network 0.119\n",
            "string 0.103\n",
            "human 0.103\n",
            "consistent 0.096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[120]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Tvx4rWXJj8jM",
        "outputId": "aa490194-5747-4f03-a5d6-07fe78d0f4cf"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'human reading curse dimensionality gale martin mcc austin galem mcc com abstract whereas optical character recognition ocr system learn classify single character people learn classify long character string parallel within single fixation difference surprising high dimensionality associated poor classification learning paper suggests human reading system avoids problem number classified image reduced consistent optimal eye fixation position character sequence regularity interesting difference exists human reading optical character recognition ocr system input output dimensionality character classification human reading much greater ocr system see ocr system classify character time human reading system classifies many character per eye fixation rayner within fixation character category sequence information extracted parallel blanchard mcconkie zola wolverton reicher ocr low dbnensionality idorothy lived hunlan reading high dbnensionality dorothy lived idorothy midst dorothy lived lil ilived imidst midst character classification versus character sequence classification interesting difference high dimensionality associated poor classification learning called curse dimensionality denker ali geman bienenstock doursat ocr system designed classify single character minimize problem fact people learn read quite well even high dimensional input output implies variance martin somehow lowered domain thereby making accurate classification learning possible present paper report simulation parallel character classification suggest variance lowered regularity eye fixation position character sequence making valid word training testing material training testing material drawn story wonderful wizard frank baum image text line created page text character total word different word divided different font case condition page different font variable constant width font different case upper case mixed case character used text line image normalized respect height width training test set contained equal mix font case condition generalization set used test crossvalidation consisted character dorothy lived jdidst great kansa prairie dorothy lived midst great kansa prairie dorothy lived midst great kansa prairie dorothy lived midst great kansa prairie dorothy ved dst great kansa pra dorothy lived midst great kansa prairie sample type font case condition used simulation network architecture simulation used backpropagation network rumelhart hinton williams extended local receptive field shared weight architecture used many character based ocr neural network lecun martin pittman previous single character based approach input net single character output vector representing category ofthe character hidden node local receptive field receive input spatially local region area preceding layer group hidden node share weight corresponding weight receptive field initialized value updated value different hidden node within group learn detect feature different location group depicted hidden node within single plane cube corresponds hidden layer different group occupy different plane cube learn detect different feature architecture bias learning reducing number free parameter available representing function fact net usually train generalize well domain local feature detector emerge similar oriented edge line detector found mammalian visual cortex hubel wiesel suggests bias least roughly appropriate extension character network character sequence network illustrated number classified character equal output node represents character category nth ordinal position first character left size input window expanded horizontally cover least widest character wwww character string made relatively narrow character character appear input window network must learn ignore human reading curse dimensionality increasing input output dimensionality accomplished expanding number hidden node horizontally network capacity described depth hidden layer number different feature detected well width hidden layer spatial coverage network network potentially sensitive local global visual information local receptive field build sensitivity letter feature shared weight make learning transfer possible across representation character different position output node globally connected node second hidden layer another word level representation network trained training set accuracy failed improve least epoch overfitting became evident periodic testing generalization test set abc efohijklmnop rstuvwxyz abcdefohijklmn pqrstuvwxyz local sharr weight ceptive ielcls net architecture parallel character sequence classification char effect dimensionality training difficulty generalization experiment provides baseline measure impact dimensionality increase dimensionality exponential increase number input output pattern number mapping function training problem arise due limitation network capacity search scope generalization problem arise becomes impractical use training set enough obtain good estimate underlying function different level dimensionality used see input window pixel beclassified character window classified character input pattern generated starting window left edge text line first character centered pixel left window successively scanning across text line character position training set size used sample relative network capacity used different feature detector per hidden layer forty different character loo low character character character ldocol dar lorotij oro dorothl dora dimensionality iorothy orot high level input output dimensionality used experiment martin network trained combination dimensionality training set size relative network capacity training difficulty described asymptotic accuracy achieved training set amount training required reach asymptote generalization reported test set used check overfitting cross validation set result see consistent higher capdy net lower capa ity net lxl ild xdj illll lxl ild xdj illll training set size training set size amount training required reach asymptote geoeralization atturiii test set lxl xdj illll lxl ild xdj oeoeralization uraq validation set croj xdj djl lxl ild xdj impact dimensionality training generalization expectation increasing dimensionality result increased training difficulty lower generalization since problem associated high dimensionality occur training test set seem alleviated somewhat high capacity net presumably due capacity search limitation insufficient size regularity window positioning way human reading might reduce problem associated high dimensionality constrain eye fixation position reading thereby reducing number different input image system must learn classify eye movement human reading curse dimensionality study suggest although fixation position within word vary consistency rayner moreover particular location fixated slightly left middle word appear optimal people efficient recognizing word location regan jacob fixation position reduce variance reducing average variability position ordered character within word position variability increase function distance fixated character average distance character within word minimized fixation position toward center word compared beginning end word experiment simulated consistent optimal positioning input window fixated character word character fixated see network learned classify first character word condition compared consistent positioning condition input window fixated first character word control condition examined replication ofthe character character condition experiment except first case network trained tested first character word second case network trained tested window fixated first character word level training set size used replication training set size window condition run network trained tested network employed different feature detector hidden layer result see support idea consistent optimal char idofthi doro consistent char roth doro high dim control char low dim control char oroth doro live window positioning dimensionality manipulation experiment consistent optimal positioning reduces variance indicated reduction training difficulty improved generalization consistent optimal positioning network achieved training generalization result superior high dimensionality control condition equivalent better low dimensionality control slightly better consistent positioning net character sequence regularity since certain character sequence allowed word character sequence regularity word may reduce number distinct image system must learn classify system may reduce variance optimizing accuracy highest frequency word hypothesis tested determining whether consistent optimal positioning network trained largest training set experiment accurate classifying high frequency word compared low frequency word accurate classifying word compared pronounceable non word random character string control condition used network trained low dimensional control character condition experiment human reading exhibit increased efficiency accuracy classifying high frequency compared low frequency word martin tralnln dlmculty amount training reach asymptote asymptotic accuracy tralning set oof consist cnlrl cnlrl itaining set size generalization acclll acy test set irl ccj isist opt consist validation set xconslsl opt ccj islst cnlrl ____ ____ cntrl box ___ cntrl box ooj ooj ooj ooj training set size itaining set size impact consistent optimal window position howe solomon solomon postman classifying character word compared pronounceable non word random character string baron thurston reicher experiment involved creating list letter word drawn text occurred frequently text said occurred infrequently paid creating list letter pronounceable non word told list letter random string sdia string reproduced font icase condition labeled create test set condition involved creating version word list case character altern ated psychologist used manipulation demonstrate advantage processing word simply due use word shape feature detector since word advantage carry alternating case condition destroys word level feature mcclelland consistent human reading see character sequence based network accurate high frequency word least accurate low frequency word character sequence based network showed progressive decline accuracy character string became le word like advantage word like string due use word shape feature detector accuracy alternating case word word shape unfamiliar remains quite high word frequency effect hgh fraq low fraq character sequence based consistent optimal positioning word superiority effect word pmn nonworcts random alternating control condition single character sensitivity word frequency character sequence regularity human reading curse dimensionality present result raise question role played high dimensionality determining reading disability difficulty reading difficulty associated reduced perceptual span rayner rayner irregular eye fixation pattern rayner pollatsek suggests reading difficulty disorder may related problem generating precise eye movement necessary maintain consistent optimal eye fixation generally result highlight importance considering role character classification learning read particularly since content factor word frequency appear influence even low level classification operation reference blanchard mcconkie zola wolverton time course visual information utilization fixation reading jour exp psych human perc perf denker schwartz wittner solla howard jackel hopfield automatic learning rule extraction generalization complex system geman bienenstock doursat neural network bias variance dilemma neural computation howe solomon visual duration threshold function word probability journal exp psych hubel wiesel brain mechanism vision sci amer lecun boser denker henderson howard hubbard jackel handwritten digit recognition backpropagation network adv neural inf proc sys touretzky morgan kaufmann martin pittman recognizing hand printed letter digit backpropagation learning neural computation mcclelland preliminary letter identification perception word nonwords jour exp psych human perc perf regan jacob optimal viewing position effect word recognition jour exp psych human perc perf rayner eye movement perceptual span beginning skilled reader jour exp child psych rayner eye guidance reading perception rayner murphy henderson pollatsek selective attentional dyslexia cognitive neuropsych rayner pollatsek psychology reading prentice hall reicher perceptual recognition function meaningfulness stimulus material jour exp psych rumelhart hinton williams learning internal representation error propagation rumelhart mcclelland ed parallel distributed processing mit press solomon postman frequency usage determinant recognition threshold word jour exp psych'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    }
  ]
}